{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOH58Nze4qqYeEf+jHA1eD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishamghimire5/Compiler_Lab/blob/main/Compiler_lab_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Given a simple program (Example: sum of numbers or Fibonacci number calculation or any simple program), write a program to identify the tokens. Your program should open the text file and scan every tokens from that program and tabulate it."
      ],
      "metadata": {
        "id": "H2MN0ojDzW38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "# Create the simple program as a string\n",
        "simple_program = '''\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    else:\n",
        "        return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "result = fibonacci(10)\n",
        "print(\"The 10th Fibonacci number is:\", result)\n",
        "'''\n",
        "\n",
        "# Write the program to a file\n",
        "filename = \"simple_program.txt\"\n",
        "with open(filename, 'w') as file:\n",
        "    file.write(simple_program)\n",
        "\n",
        "def tokenize(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Define token patterns\n",
        "    patterns = [\n",
        "        ('KEYWORD', r'\\b(def|if|else|return|print)\\b'),\n",
        "        ('IDENTIFIER', r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'),\n",
        "        ('OPERATOR', r'[+\\-*/=():,<>]'),\n",
        "        ('LITERAL', r'\\b\\d+\\b|\"[^\"]*\"'),\n",
        "        ('WHITESPACE', r'\\s+')\n",
        "    ]\n",
        "\n",
        "    tokens = []\n",
        "    pos = 0\n",
        "    while pos < len(content):\n",
        "        match = None\n",
        "        for token_type, pattern in patterns:\n",
        "            regex = re.compile(pattern)\n",
        "            match = regex.match(content, pos)\n",
        "            if match:\n",
        "                if token_type != 'WHITESPACE':\n",
        "                    tokens.append((token_type, match.group(0)))\n",
        "                pos = match.end()\n",
        "                break\n",
        "        if not match:\n",
        "            print(f\"Error: Unexpected character '{content[pos]}' at position {pos}\")\n",
        "            pos += 1\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the file\n",
        "tokens = tokenize(filename)\n",
        "\n",
        "# Display results in a table\n",
        "print(\"{:<15} {:<15}\".format(\"Token Type\", \"Value\"))\n",
        "print(\"-\" * 30)\n",
        "for token_type, value in tokens:\n",
        "    print(\"{:<15} {:<15}\".format(token_type, value))\n",
        "\n",
        "# Clean up: remove the created file\n",
        "os.remove(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaiwwdH40b0N",
        "outputId": "5a6e4ba2-f36b-4341-89c2-3a9b9a98ee99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Type      Value          \n",
            "------------------------------\n",
            "KEYWORD         def            \n",
            "IDENTIFIER      fibonacci      \n",
            "OPERATOR        (              \n",
            "IDENTIFIER      n              \n",
            "OPERATOR        )              \n",
            "OPERATOR        :              \n",
            "KEYWORD         if             \n",
            "IDENTIFIER      n              \n",
            "OPERATOR        <              \n",
            "OPERATOR        =              \n",
            "LITERAL         1              \n",
            "OPERATOR        :              \n",
            "KEYWORD         return         \n",
            "IDENTIFIER      n              \n",
            "KEYWORD         else           \n",
            "OPERATOR        :              \n",
            "KEYWORD         return         \n",
            "IDENTIFIER      fibonacci      \n",
            "OPERATOR        (              \n",
            "IDENTIFIER      n              \n",
            "OPERATOR        -              \n",
            "LITERAL         1              \n",
            "OPERATOR        )              \n",
            "OPERATOR        +              \n",
            "IDENTIFIER      fibonacci      \n",
            "OPERATOR        (              \n",
            "IDENTIFIER      n              \n",
            "OPERATOR        -              \n",
            "LITERAL         2              \n",
            "OPERATOR        )              \n",
            "IDENTIFIER      result         \n",
            "OPERATOR        =              \n",
            "IDENTIFIER      fibonacci      \n",
            "OPERATOR        (              \n",
            "LITERAL         10             \n",
            "OPERATOR        )              \n",
            "KEYWORD         print          \n",
            "OPERATOR        (              \n",
            "LITERAL         \"The 10th Fibonacci number is:\"\n",
            "OPERATOR        ,              \n",
            "IDENTIFIER      result         \n",
            "OPERATOR        )              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "In this program, we implemented a simple lexical analyzer (tokenizer) for Python code. We first created a sample Python function (Fibonacci) as a string and wrote it to a file. The tokenizer then reads this file and uses regular expressions to identify different types of tokens such as keywords, identifiers, operators, and literals. It does this by iterating through the code character by character, matching each segment against predefined patterns for different token types. The program then categorizes these tokens and stores them in a list. Finally, it displays the results in a tabular format, showing each token's type and value. This process demonstrates the first step in compiler design: lexical analysis, where the source code is broken down into its fundamental components (tokens) for further processing in subsequent compilation stages."
      ],
      "metadata": {
        "id": "dc2-OtdO1dxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Write a program to remove the left recursion from the given grammar."
      ],
      "metadata": {
        "id": "nITFN5JK1qVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Production:\n",
        "    def __init__(self, left, right):\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "def remove_left_recursion(grammar):\n",
        "    result = []\n",
        "    for i, prod in enumerate(grammar):\n",
        "        alpha = []\n",
        "        beta = []\n",
        "        for right in prod.right:\n",
        "            if right[0] == prod.left:\n",
        "                alpha.append(right[1:])\n",
        "            else:\n",
        "                beta.append(right)\n",
        "\n",
        "        if alpha:\n",
        "            new_symbol = f\"{prod.left}'\"\n",
        "            result.append(Production(prod.left, [b + new_symbol for b in beta]))\n",
        "            result.append(Production(new_symbol, [a + new_symbol for a in alpha] + ['ε']))\n",
        "        else:\n",
        "            result.append(prod)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Input grammar\n",
        "grammar = [\n",
        "    Production('E', ['E+T', 'T']),\n",
        "    Production('T', ['T*F', 'F']),\n",
        "    Production('F', ['(E)', 'id'])\n",
        "]\n",
        "\n",
        "print(\"Original Grammar:\")\n",
        "for prod in grammar:\n",
        "    print(f\"{prod.left} -> {' | '.join(prod.right)}\")\n",
        "\n",
        "new_grammar = remove_left_recursion(grammar)\n",
        "\n",
        "print(\"\\nGrammar after removing left recursion:\")\n",
        "for prod in new_grammar:\n",
        "    print(f\"{prod.left} -> {' | '.join(prod.right)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEOCrwSo1pzw",
        "outputId": "5db47051-02ed-4bbd-ce60-68c1621f3892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Grammar:\n",
            "E -> E+T | T\n",
            "T -> T*F | F\n",
            "F -> (E) | id\n",
            "\n",
            "Grammar after removing left recursion:\n",
            "E -> TE'\n",
            "E' -> +TE' | ε\n",
            "T -> FT'\n",
            "T' -> *FT' | ε\n",
            "F -> (E) | id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "This program implements an algorithm to eliminate left recursion from a context-free grammar. It works by transforming left-recursive productions into an equivalent non-left-recursive form. The program defines a `Production` class to represent grammar rules and a `remove_left_recursion` function that processes each production. For each left-recursive production, it separates the recursive and non-recursive parts (alpha and beta), then creates new productions that eliminate the left recursion. The result is an equivalent grammar without left recursion, which is more suitable for certain parsing techniques, particularly top-down parsing methods.\n",
        "In this example, we start with a simple grammar for arithmetic expressions and transform it to remove left recursion. The resulting grammar is equivalent but can be parsed more efficiently by top-down parsers."
      ],
      "metadata": {
        "id": "cF-BXG082Dx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Write a program to demonstrate left factoring from a left factored grammar."
      ],
      "metadata": {
        "id": "VqHOKVvP4PH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Production:\n",
        "    def __init__(self, left, right):\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "def left_factor(grammar):\n",
        "    result = []\n",
        "    for prod in grammar:\n",
        "        if len(prod.right) > 1 and all(r.startswith(prod.right[0][0]) for r in prod.right):\n",
        "            common_prefix = prod.right[0][0]\n",
        "            new_symbol = f\"{prod.left}'\"\n",
        "            result.append(Production(prod.left, [common_prefix + new_symbol]))\n",
        "            result.append(Production(new_symbol, [r[1:] for r in prod.right]))\n",
        "        else:\n",
        "            result.append(prod)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Input grammar\n",
        "grammar = [\n",
        "    Production('A', ['aB', 'aC', 'aD']),\n",
        "    Production('B', ['b']),\n",
        "    Production('C', ['c']),\n",
        "    Production('D', ['d'])\n",
        "]\n",
        "\n",
        "print(\"Original Grammar:\")\n",
        "for prod in grammar:\n",
        "    print(f\"{prod.left} -> {' | '.join(prod.right)}\")\n",
        "\n",
        "new_grammar = left_factor(grammar)\n",
        "\n",
        "print(\"\\nGrammar after left factoring:\")\n",
        "for prod in new_grammar:\n",
        "    print(f\"{prod.left} -> {' | '.join(prod.right)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbE37QTm4OdQ",
        "outputId": "3fc09310-8d72-40d8-c003-d08ca1147229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Grammar:\n",
            "A -> aB | aC | aD\n",
            "B -> b\n",
            "C -> c\n",
            "D -> d\n",
            "\n",
            "Grammar after left factoring:\n",
            "A -> aA'\n",
            "A' -> B | C | D\n",
            "B -> b\n",
            "C -> c\n",
            "D -> d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "This program implements left factoring, a grammar transformation technique used in compiler design. It takes a context-free grammar as input and produces an equivalent left-factored grammar. The algorithm identifies common prefixes in the productions of each non-terminal. When a common prefix is found, it creates a new non-terminal and factors out the common part. In this example, we start with a grammar that has left-factored productions for the non-terminal A. The program applies left factoring to this grammar, creating a new non-terminal A' to represent the variations after the common prefix 'a'. The resulting grammar eliminates the ambiguity for top-down parsers by ensuring that each non-terminal has at most one production starting with a given terminal symbol. This transformation is crucial for creating efficient and unambiguous parsers in compiler design."
      ],
      "metadata": {
        "id": "qHDEt3od7vIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Write a program to identify FIRST and FOLLOW from the given grammar."
      ],
      "metadata": {
        "id": "R9RIaBcV8DL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def cal_follow(s, productions, first):\n",
        "    follow = set()\n",
        "    if len(s) != 1:\n",
        "        return set()\n",
        "    if s == list(productions.keys())[0]:\n",
        "        follow.add('$')\n",
        "\n",
        "    for i in productions:\n",
        "        for j in range(len(productions[i])):\n",
        "            if s in productions[i][j]:\n",
        "                idx = productions[i][j].index(s)\n",
        "\n",
        "                if idx == len(productions[i][j]) - 1:\n",
        "                    if productions[i][j][idx] == i:\n",
        "                        break\n",
        "                    else:\n",
        "                        f = cal_follow(i, productions, first)\n",
        "                        follow.update(f)\n",
        "                else:\n",
        "                    while idx != len(productions[i][j]) - 1:\n",
        "                        idx += 1\n",
        "                        if not productions[i][j][idx].isupper():\n",
        "                            follow.add(productions[i][j][idx])\n",
        "                            break\n",
        "                        else:\n",
        "                            f = cal_first(productions[i][j][idx], productions)\n",
        "\n",
        "                            if 'ε' not in f:\n",
        "                                follow.update(f)\n",
        "                                break\n",
        "                            elif 'ε' in f and idx != len(productions[i][j]) - 1:\n",
        "                                f.discard('ε')\n",
        "                                follow.update(f)\n",
        "                            elif 'ε' in f and idx == len(productions[i][j]) - 1:\n",
        "                                f.discard('ε')\n",
        "                                follow.update(f)\n",
        "                                f = cal_follow(i, productions, first)\n",
        "                                follow.update(f)\n",
        "    return follow\n",
        "\n",
        "def cal_first(s, productions):\n",
        "    first = set()\n",
        "\n",
        "    for i in range(len(productions[s])):\n",
        "        for j in range(len(productions[s][i])):\n",
        "            c = productions[s][i][j]\n",
        "            if c.isupper():\n",
        "                f = cal_first(c, productions)\n",
        "                if 'ε' not in f:\n",
        "                    first.update(f)\n",
        "                    break\n",
        "                else:\n",
        "                    if j == len(productions[s][i]) - 1:\n",
        "                        first.update(f)\n",
        "                    else:\n",
        "                        f.discard('ε')\n",
        "                        first.update(f)\n",
        "            else:\n",
        "                first.add(c)\n",
        "                break\n",
        "\n",
        "    return first\n",
        "\n",
        "def main():\n",
        "    productions = {\n",
        "        \"X\": [[\"T\", \"n\", \"S\"], [\"R\", \"m\"]],\n",
        "        \"T\": [[\"q\"], [\"#\"]],\n",
        "        \"S\": [[\"p\"], [\"#\"]],\n",
        "        \"R\": [[\"o\", \"m\"], [\"S\", \"T\"]]\n",
        "    }\n",
        "\n",
        "    first = {}\n",
        "    follow = {}\n",
        "\n",
        "    for s in productions.keys():\n",
        "        first[s] = cal_first(s, productions)\n",
        "\n",
        "    print(\"*****FIRST*****\")\n",
        "    for lhs, rhs in first.items():\n",
        "        print(lhs, \":\", rhs)\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    for lhs in productions:\n",
        "        follow[lhs] = set()\n",
        "\n",
        "    for s in productions.keys():\n",
        "        follow[s] = cal_follow(s, productions, first)\n",
        "\n",
        "    print(\"*****FOLLOW*****\")\n",
        "    for lhs, rhs in follow.items():\n",
        "        print(lhs, \":\", rhs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Lyenp-c7wLG",
        "outputId": "b6a93600-2d5d-460a-beac-da56cc5690e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****FIRST*****\n",
            "X : {'q', 'p', 'o', '#'}\n",
            "T : {'q', '#'}\n",
            "S : {'#', 'p'}\n",
            "R : {'o', '#', 'p'}\n",
            "\n",
            "*****FOLLOW*****\n",
            "X : {'$'}\n",
            "T : {'n', 'm'}\n",
            "S : {'q', '$', '#'}\n",
            "R : {'m'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "This Python program calculates the FIRST and FOLLOW sets for a given context-free grammar. The `cal_first` function recursively computes the FIRST set for each non-terminal, handling epsilon productions and concatenation. The `cal_follow` function calculates the FOLLOW set for each non-terminal, considering the positions of symbols in productions and using the FIRST sets where necessary. It handles special cases like the start symbol and end-of-production scenarios. The main function defines the grammar as a dictionary of productions, computes FIRST and FOLLOW sets for all non-terminals, and prints the results. This implementation accurately handles complex grammar structures, including epsilon productions and indirect derivations, providing a comprehensive solution for FIRST and FOLLOW set computation in compiler design."
      ],
      "metadata": {
        "id": "ENXShjLsCMpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Write a program to simulate the parsing process of LL(1) grammar. Take necessary measure to use parsing table."
      ],
      "metadata": {
        "id": "4pHP3MZTCTcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LL(1) parser code in python\n",
        "\n",
        "def removeLeftRecursion(rulesDiction):\n",
        "\t# for rule: A->Aa|b\n",
        "\t# result: A->bA',A'->aA'|#\n",
        "\n",
        "\t# 'store' has new rules to be added\n",
        "\tstore = {}\n",
        "\t# traverse over rules\n",
        "\tfor lhs in rulesDiction:\n",
        "\t\t# alphaRules stores subrules with left-recursion\n",
        "\t\t# betaRules stores subrules without left-recursion\n",
        "\t\talphaRules = []\n",
        "\t\tbetaRules = []\n",
        "\t\t# get rhs for current lhs\n",
        "\t\tallrhs = rulesDiction[lhs]\n",
        "\t\tfor subrhs in allrhs:\n",
        "\t\t\tif subrhs[0] == lhs:\n",
        "\t\t\t\talphaRules.append(subrhs[1:])\n",
        "\t\t\telse:\n",
        "\t\t\t\tbetaRules.append(subrhs)\n",
        "\t\t# alpha and beta containing subrules are separated\n",
        "\t\t# now form two new rules\n",
        "\t\tif len(alphaRules) != 0:\n",
        "\t\t\t# to generate new unique symbol\n",
        "\t\t\t# add ' till unique not generated\n",
        "\t\t\tlhs_ = lhs + \"'\"\n",
        "\t\t\twhile (lhs_ in rulesDiction.keys()) \\\n",
        "\t\t\t\t\tor (lhs_ in store.keys()):\n",
        "\t\t\t\tlhs_ += \"'\"\n",
        "\t\t\t# make beta rule\n",
        "\t\t\tfor b in range(0, len(betaRules)):\n",
        "\t\t\t\tbetaRules[b].append(lhs_)\n",
        "\t\t\trulesDiction[lhs] = betaRules\n",
        "\t\t\t# make alpha rule\n",
        "\t\t\tfor a in range(0, len(alphaRules)):\n",
        "\t\t\t\talphaRules[a].append(lhs_)\n",
        "\t\t\talphaRules.append(['#'])\n",
        "\t\t\t# store in temp dict, append to\n",
        "\t\t\t# - rulesDiction at end of traversal\n",
        "\t\t\tstore[lhs_] = alphaRules\n",
        "\t# add newly generated rules generated\n",
        "\t# - after removing left recursion\n",
        "\tfor left in store:\n",
        "\t\trulesDiction[left] = store[left]\n",
        "\treturn rulesDiction\n",
        "\n",
        "\n",
        "def LeftFactoring(rulesDiction):\n",
        "\t# for rule: A->aDF|aCV|k\n",
        "\t# result: A->aA'|k, A'->DF|CV\n",
        "\n",
        "\t# newDict stores newly generated\n",
        "\t# - rules after left factoring\n",
        "\tnewDict = {}\n",
        "\t# iterate over all rules of dictionary\n",
        "\tfor lhs in rulesDiction:\n",
        "\t\t# get rhs for given lhs\n",
        "\t\tallrhs = rulesDiction[lhs]\n",
        "\t\t# temp dictionary helps detect left factoring\n",
        "\t\ttemp = dict()\n",
        "\t\tfor subrhs in allrhs:\n",
        "\t\t\tif subrhs[0] not in list(temp.keys()):\n",
        "\t\t\t\ttemp[subrhs[0]] = [subrhs]\n",
        "\t\t\telse:\n",
        "\t\t\t\ttemp[subrhs[0]].append(subrhs)\n",
        "\t\t# if value list count for any key in temp is > 1,\n",
        "\t\t# - it has left factoring\n",
        "\t\t# new_rule stores new subrules for current LHS symbol\n",
        "\t\tnew_rule = []\n",
        "\t\t# temp_dict stores new subrules for left factoring\n",
        "\t\ttempo_dict = {}\n",
        "\t\tfor term_key in temp:\n",
        "\t\t\t# get value from temp for term_key\n",
        "\t\t\tallStartingWithTermKey = temp[term_key]\n",
        "\t\t\tif len(allStartingWithTermKey) > 1:\n",
        "\t\t\t\t# left factoring required\n",
        "\t\t\t\t# to generate new unique symbol\n",
        "\t\t\t\t# - add ' till unique not generated\n",
        "\t\t\t\tlhs_ = lhs + \"'\"\n",
        "\t\t\t\twhile (lhs_ in rulesDiction.keys()) \\\n",
        "\t\t\t\t\t\tor (lhs_ in tempo_dict.keys()):\n",
        "\t\t\t\t\tlhs_ += \"'\"\n",
        "\t\t\t\t# append the left factored result\n",
        "\t\t\t\tnew_rule.append([term_key, lhs_])\n",
        "\t\t\t\t# add expanded rules to tempo_dict\n",
        "\t\t\t\tex_rules = []\n",
        "\t\t\t\tfor g in temp[term_key]:\n",
        "\t\t\t\t\tex_rules.append(g[1:])\n",
        "\t\t\t\ttempo_dict[lhs_] = ex_rules\n",
        "\t\t\telse:\n",
        "\t\t\t\t# no left factoring required\n",
        "\t\t\t\tnew_rule.append(allStartingWithTermKey[0])\n",
        "\t\t# add original rule\n",
        "\t\tnewDict[lhs] = new_rule\n",
        "\t\t# add newly generated rules after left factoring\n",
        "\t\tfor key in tempo_dict:\n",
        "\t\t\tnewDict[key] = tempo_dict[key]\n",
        "\treturn newDict\n",
        "\n",
        "\n",
        "# calculation of first\n",
        "# epsilon is denoted by '#' (semi-colon)\n",
        "\n",
        "# pass rule in first function\n",
        "def first(rule):\n",
        "\tglobal rules, nonterm_userdef, \\\n",
        "\t\tterm_userdef, diction, firsts\n",
        "\t# recursion base condition\n",
        "\t# (for terminal or epsilon)\n",
        "\tif len(rule) != 0 and (rule is not None):\n",
        "\t\tif rule[0] in term_userdef:\n",
        "\t\t\treturn rule[0]\n",
        "\t\telif rule[0] == '#':\n",
        "\t\t\treturn '#'\n",
        "\n",
        "\t# condition for Non-Terminals\n",
        "\tif len(rule) != 0:\n",
        "\t\tif rule[0] in list(diction.keys()):\n",
        "\t\t\t# fres temporary list of result\n",
        "\t\t\tfres = []\n",
        "\t\t\trhs_rules = diction[rule[0]]\n",
        "\t\t\t# call first on each rule of RHS\n",
        "\t\t\t# fetched (& take union)\n",
        "\t\t\tfor itr in rhs_rules:\n",
        "\t\t\t\tindivRes = first(itr)\n",
        "\t\t\t\tif type(indivRes) is list:\n",
        "\t\t\t\t\tfor i in indivRes:\n",
        "\t\t\t\t\t\tfres.append(i)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tfres.append(indivRes)\n",
        "\n",
        "\t\t\t# if no epsilon in result\n",
        "\t\t\t# - received return fres\n",
        "\t\t\tif '#' not in fres:\n",
        "\t\t\t\treturn fres\n",
        "\t\t\telse:\n",
        "\t\t\t\t# apply epsilon\n",
        "\t\t\t\t# rule => f(ABC)=f(A)-{e} U f(BC)\n",
        "\t\t\t\tnewList = []\n",
        "\t\t\t\tfres.remove('#')\n",
        "\t\t\t\tif len(rule) > 1:\n",
        "\t\t\t\t\tansNew = first(rule[1:])\n",
        "\t\t\t\t\tif ansNew != None:\n",
        "\t\t\t\t\t\tif type(ansNew) is list:\n",
        "\t\t\t\t\t\t\tnewList = fres + ansNew\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tnewList = fres + [ansNew]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnewList = fres\n",
        "\t\t\t\t\treturn newList\n",
        "\t\t\t\t# if result is not already returned\n",
        "\t\t\t\t# - control reaches here\n",
        "\t\t\t\t# lastly if eplison still persists\n",
        "\t\t\t\t# - keep it in result of first\n",
        "\t\t\t\tfres.append('#')\n",
        "\t\t\t\treturn fres\n",
        "\n",
        "\n",
        "# calculation of follow\n",
        "# use 'rules' list, and 'diction' dict from above\n",
        "\n",
        "# follow function input is the split result on\n",
        "# - Non-Terminal whose Follow we want to compute\n",
        "def follow(nt):\n",
        "\tglobal start_symbol, rules, nonterm_userdef, \\\n",
        "\t\tterm_userdef, diction, firsts, follows\n",
        "\t# for start symbol return $ (recursion base case)\n",
        "\n",
        "\tsolset = set()\n",
        "\tif nt == start_symbol:\n",
        "\t\t# return '$'\n",
        "\t\tsolset.add('$')\n",
        "\n",
        "\t# check all occurrences\n",
        "\t# solset - is result of computed 'follow' so far\n",
        "\n",
        "\t# For input, check in all rules\n",
        "\tfor curNT in diction:\n",
        "\t\trhs = diction[curNT]\n",
        "\t\t# go for all productions of NT\n",
        "\t\tfor subrule in rhs:\n",
        "\t\t\tif nt in subrule:\n",
        "\t\t\t\t# call for all occurrences on\n",
        "\t\t\t\t# - non-terminal in subrule\n",
        "\t\t\t\twhile nt in subrule:\n",
        "\t\t\t\t\tindex_nt = subrule.index(nt)\n",
        "\t\t\t\t\tsubrule = subrule[index_nt + 1:]\n",
        "\t\t\t\t\t# empty condition - call follow on LHS\n",
        "\t\t\t\t\tif len(subrule) != 0:\n",
        "\t\t\t\t\t\t# compute first if symbols on\n",
        "\t\t\t\t\t\t# - RHS of target Non-Terminal exists\n",
        "\t\t\t\t\t\tres = first(subrule)\n",
        "\t\t\t\t\t\t# if epsilon in result apply rule\n",
        "\t\t\t\t\t\t# - (A->aBX)- follow of -\n",
        "\t\t\t\t\t\t# - follow(B)=(first(X)-{ep}) U follow(A)\n",
        "\t\t\t\t\t\tif '#' in res:\n",
        "\t\t\t\t\t\t\tnewList = []\n",
        "\t\t\t\t\t\t\tres.remove('#')\n",
        "\t\t\t\t\t\t\tansNew = follow(curNT)\n",
        "\t\t\t\t\t\t\tif ansNew != None:\n",
        "\t\t\t\t\t\t\t\tif type(ansNew) is list:\n",
        "\t\t\t\t\t\t\t\t\tnewList = res + ansNew\n",
        "\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\tnewList = res + [ansNew]\n",
        "\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\tnewList = res\n",
        "\t\t\t\t\t\t\tres = newList\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t# when nothing in RHS, go circular\n",
        "\t\t\t\t\t\t# - and take follow of LHS\n",
        "\t\t\t\t\t\t# only if (NT in LHS)!=curNT\n",
        "\t\t\t\t\t\tif nt != curNT:\n",
        "\t\t\t\t\t\t\tres = follow(curNT)\n",
        "\n",
        "\t\t\t\t\t# add follow result in set form\n",
        "\t\t\t\t\tif res is not None:\n",
        "\t\t\t\t\t\tif type(res) is list:\n",
        "\t\t\t\t\t\t\tfor g in res:\n",
        "\t\t\t\t\t\t\t\tsolset.add(g)\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tsolset.add(res)\n",
        "\treturn list(solset)\n",
        "\n",
        "\n",
        "def computeAllFirsts():\n",
        "\tglobal rules, nonterm_userdef, \\\n",
        "\t\tterm_userdef, diction, firsts\n",
        "\tfor rule in rules:\n",
        "\t\tk = rule.split(\"->\")\n",
        "\t\t# remove un-necessary spaces\n",
        "\t\tk[0] = k[0].strip()\n",
        "\t\tk[1] = k[1].strip()\n",
        "\t\trhs = k[1]\n",
        "\t\tmultirhs = rhs.split('|')\n",
        "\t\t# remove un-necessary spaces\n",
        "\t\tfor i in range(len(multirhs)):\n",
        "\t\t\tmultirhs[i] = multirhs[i].strip()\n",
        "\t\t\tmultirhs[i] = multirhs[i].split()\n",
        "\t\tdiction[k[0]] = multirhs\n",
        "\n",
        "\tprint(f\"\\nRules: \\n\")\n",
        "\tfor y in diction:\n",
        "\t\tprint(f\"{y}->{diction[y]}\")\n",
        "\tprint(f\"\\nAfter elimination of left recursion:\\n\")\n",
        "\n",
        "\tdiction = removeLeftRecursion(diction)\n",
        "\tfor y in diction:\n",
        "\t\tprint(f\"{y}->{diction[y]}\")\n",
        "\tprint(\"\\nAfter left factoring:\\n\")\n",
        "\n",
        "\tdiction = LeftFactoring(diction)\n",
        "\tfor y in diction:\n",
        "\t\tprint(f\"{y}->{diction[y]}\")\n",
        "\n",
        "\t# calculate first for each rule\n",
        "\t# - (call first() on all RHS)\n",
        "\tfor y in list(diction.keys()):\n",
        "\t\tt = set()\n",
        "\t\tfor sub in diction.get(y):\n",
        "\t\t\tres = first(sub)\n",
        "\t\t\tif res != None:\n",
        "\t\t\t\tif type(res) is list:\n",
        "\t\t\t\t\tfor u in res:\n",
        "\t\t\t\t\t\tt.add(u)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tt.add(res)\n",
        "\n",
        "\t\t# save result in 'firsts' list\n",
        "\t\tfirsts[y] = t\n",
        "\n",
        "\tprint(\"\\nCalculated firsts: \")\n",
        "\tkey_list = list(firsts.keys())\n",
        "\tindex = 0\n",
        "\tfor gg in firsts:\n",
        "\t\tprint(f\"first({key_list[index]}) \"\n",
        "\t\t\tf\"=> {firsts.get(gg)}\")\n",
        "\t\tindex += 1\n",
        "\n",
        "\n",
        "def computeAllFollows():\n",
        "\tglobal start_symbol, rules, nonterm_userdef,\\\n",
        "\t\tterm_userdef, diction, firsts, follows\n",
        "\tfor NT in diction:\n",
        "\t\tsolset = set()\n",
        "\t\tsol = follow(NT)\n",
        "\t\tif sol is not None:\n",
        "\t\t\tfor g in sol:\n",
        "\t\t\t\tsolset.add(g)\n",
        "\t\tfollows[NT] = solset\n",
        "\n",
        "\tprint(\"\\nCalculated follows: \")\n",
        "\tkey_list = list(follows.keys())\n",
        "\tindex = 0\n",
        "\tfor gg in follows:\n",
        "\t\tprint(f\"follow({key_list[index]})\"\n",
        "\t\t\tf\" => {follows[gg]}\")\n",
        "\t\tindex += 1\n",
        "\n",
        "\n",
        "# create parse table\n",
        "def createParseTable():\n",
        "\timport copy\n",
        "\tglobal diction, firsts, follows, term_userdef\n",
        "\tprint(\"\\nFirsts and Follow Result table\\n\")\n",
        "\n",
        "\t# find space size\n",
        "\tmx_len_first = 0\n",
        "\tmx_len_fol = 0\n",
        "\tfor u in diction:\n",
        "\t\tk1 = len(str(firsts[u]))\n",
        "\t\tk2 = len(str(follows[u]))\n",
        "\t\tif k1 > mx_len_first:\n",
        "\t\t\tmx_len_first = k1\n",
        "\t\tif k2 > mx_len_fol:\n",
        "\t\t\tmx_len_fol = k2\n",
        "\n",
        "\tprint(f\"{{:<{10}}} \"\n",
        "\t\tf\"{{:<{mx_len_first + 5}}} \"\n",
        "\t\tf\"{{:<{mx_len_fol + 5}}}\"\n",
        "\t\t.format(\"Non-T\", \"FIRST\", \"FOLLOW\"))\n",
        "\tfor u in diction:\n",
        "\t\tprint(f\"{{:<{10}}} \"\n",
        "\t\t\tf\"{{:<{mx_len_first + 5}}} \"\n",
        "\t\t\tf\"{{:<{mx_len_fol + 5}}}\"\n",
        "\t\t\t.format(u, str(firsts[u]), str(follows[u])))\n",
        "\n",
        "\t# create matrix of row(NT) x [col(T) + 1($)]\n",
        "\t# create list of non-terminals\n",
        "\tntlist = list(diction.keys())\n",
        "\tterminals = copy.deepcopy(term_userdef)\n",
        "\tterminals.append('$')\n",
        "\n",
        "\t# create the initial empty state of ,matrix\n",
        "\tmat = []\n",
        "\tfor x in diction:\n",
        "\t\trow = []\n",
        "\t\tfor y in terminals:\n",
        "\t\t\trow.append('')\n",
        "\t\t# of $ append one more col\n",
        "\t\tmat.append(row)\n",
        "\n",
        "\t# Classifying grammar as LL(1) or not LL(1)\n",
        "\tgrammar_is_LL = True\n",
        "\n",
        "\t# rules implementation\n",
        "\tfor lhs in diction:\n",
        "\t\trhs = diction[lhs]\n",
        "\t\tfor y in rhs:\n",
        "\t\t\tres = first(y)\n",
        "\t\t\t# epsilon is present,\n",
        "\t\t\t# - take union with follow\n",
        "\t\t\tif '#' in res:\n",
        "\t\t\t\tif type(res) == str:\n",
        "\t\t\t\t\tfirstFollow = []\n",
        "\t\t\t\t\tfol_op = follows[lhs]\n",
        "\t\t\t\t\tif fol_op is str:\n",
        "\t\t\t\t\t\tfirstFollow.append(fol_op)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tfor u in fol_op:\n",
        "\t\t\t\t\t\t\tfirstFollow.append(u)\n",
        "\t\t\t\t\tres = firstFollow\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tres.remove('#')\n",
        "\t\t\t\t\tres = list(res) +\\\n",
        "\t\t\t\t\t\tlist(follows[lhs])\n",
        "\t\t\t# add rules to table\n",
        "\t\t\tttemp = []\n",
        "\t\t\tif type(res) is str:\n",
        "\t\t\t\tttemp.append(res)\n",
        "\t\t\t\tres = copy.deepcopy(ttemp)\n",
        "\t\t\tfor c in res:\n",
        "\t\t\t\txnt = ntlist.index(lhs)\n",
        "\t\t\t\tyt = terminals.index(c)\n",
        "\t\t\t\tif mat[xnt][yt] == '':\n",
        "\t\t\t\t\tmat[xnt][yt] = mat[xnt][yt] \\\n",
        "\t\t\t\t\t\t\t\t+ f\"{lhs}->{' '.join(y)}\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# if rule already present\n",
        "\t\t\t\t\tif f\"{lhs}->{y}\" in mat[xnt][yt]:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tgrammar_is_LL = False\n",
        "\t\t\t\t\t\tmat[xnt][yt] = mat[xnt][yt] \\\n",
        "\t\t\t\t\t\t\t\t\t+ f\",{lhs}->{' '.join(y)}\"\n",
        "\n",
        "\t# final state of parse table\n",
        "\tprint(\"\\nGenerated parsing table:\\n\")\n",
        "\tfrmt = \"{:>12}\" * len(terminals)\n",
        "\tprint(frmt.format(*terminals))\n",
        "\n",
        "\tj = 0\n",
        "\tfor y in mat:\n",
        "\t\tfrmt1 = \"{:>12}\" * len(y)\n",
        "\t\tprint(f\"{ntlist[j]} {frmt1.format(*y)}\")\n",
        "\t\tj += 1\n",
        "\n",
        "\treturn (mat, grammar_is_LL, terminals)\n",
        "\n",
        "\n",
        "def validateStringUsingStackBuffer(parsing_table, grammarll1,\n",
        "\t\t\t\t\t\t\t\ttable_term_list, input_string,\n",
        "\t\t\t\t\t\t\t\tterm_userdef,start_symbol):\n",
        "\n",
        "\tprint(f\"\\nValidate String => {input_string}\\n\")\n",
        "\n",
        "\t# for more than one entries\n",
        "\t# - in one cell of parsing table\n",
        "\tif grammarll1 == False:\n",
        "\t\treturn f\"\\nInput String = \" \\\n",
        "\t\t\tf\"\\\"{input_string}\\\"\\n\" \\\n",
        "\t\t\tf\"Grammar is not LL(1)\"\n",
        "\n",
        "\t# implementing stack buffer\n",
        "\n",
        "\tstack = [start_symbol, '$']\n",
        "\tbuffer = []\n",
        "\n",
        "\t# reverse input string store in buffer\n",
        "\tinput_string = input_string.split()\n",
        "\tinput_string.reverse()\n",
        "\tbuffer = ['$'] + input_string\n",
        "\n",
        "\tprint(\"{:>20} {:>20} {:>20}\".\n",
        "\t\tformat(\"Buffer\", \"Stack\",\"Action\"))\n",
        "\n",
        "\twhile True:\n",
        "\t\t# end loop if all symbols matched\n",
        "\t\tif stack == ['$'] and buffer == ['$']:\n",
        "\t\t\tprint(\"{:>20} {:>20} {:>20}\"\n",
        "\t\t\t\t.format(' '.join(buffer),\n",
        "\t\t\t\t\t\t' '.join(stack),\n",
        "\t\t\t\t\t\t\"Valid\"))\n",
        "\t\t\treturn \"\\nValid String!\"\n",
        "\t\telif stack[0] not in term_userdef:\n",
        "\t\t\t# take font of buffer (y) and tos (x)\n",
        "\t\t\tx = list(diction.keys()).index(stack[0])\n",
        "\t\t\ty = table_term_list.index(buffer[-1])\n",
        "\t\t\tif parsing_table[x][y] != '':\n",
        "\t\t\t\t# format table entry received\n",
        "\t\t\t\tentry = parsing_table[x][y]\n",
        "\t\t\t\tprint(\"{:>20} {:>20} {:>25}\".\n",
        "\t\t\t\t\tformat(' '.join(buffer),\n",
        "\t\t\t\t\t\t\t' '.join(stack),\n",
        "\t\t\t\t\t\t\tf\"T[{stack[0]}][{buffer[-1]}] = {entry}\"))\n",
        "\t\t\t\tlhs_rhs = entry.split(\"->\")\n",
        "\t\t\t\tlhs_rhs[1] = lhs_rhs[1].replace('#', '').strip()\n",
        "\t\t\t\tentryrhs = lhs_rhs[1].split()\n",
        "\t\t\t\tstack = entryrhs + stack[1:]\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn f\"\\nInvalid String! No rule at \" \\\n",
        "\t\t\t\t\tf\"Table[{stack[0]}][{buffer[-1]}].\"\n",
        "\t\telse:\n",
        "\t\t\t# stack top is Terminal\n",
        "\t\t\tif stack[0] == buffer[-1]:\n",
        "\t\t\t\tprint(\"{:>20} {:>20} {:>20}\"\n",
        "\t\t\t\t\t.format(' '.join(buffer),\n",
        "\t\t\t\t\t\t\t' '.join(stack),\n",
        "\t\t\t\t\t\t\tf\"Matched:{stack[0]}\"))\n",
        "\t\t\t\tbuffer = buffer[:-1]\n",
        "\t\t\t\tstack = stack[1:]\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn \"\\nInvalid String! \" \\\n",
        "\t\t\t\t\t\"Unmatched terminal symbols\"\n",
        "\n",
        "# sample set (left factoring & recursion present)\n",
        "rules=[\"S -> A k O\",\n",
        "\t\"A -> A d | a B | a C\",\n",
        "\t\"C -> c\",\n",
        "\t\"B -> b B C | r\"]\n",
        "\n",
        "nonterm_userdef=['A','B','C']\n",
        "term_userdef=['k','O','d','a','c','b','r']\n",
        "sample_input_string=\"a r k O\"\n",
        "\n",
        "# diction - store rules inputted\n",
        "# firsts - store computed firsts\n",
        "diction = {}\n",
        "firsts = {}\n",
        "follows = {}\n",
        "\n",
        "# computes all FIRSTs for all non terminals\n",
        "computeAllFirsts()\n",
        "# assuming first rule has start_symbol\n",
        "# start symbol can be modified in below line of code\n",
        "start_symbol = list(diction.keys())[0]\n",
        "# computes all FOLLOWs for all occurrences\n",
        "computeAllFollows()\n",
        "# generate formatted first and follow table\n",
        "# then generate parse table\n",
        "\n",
        "(parsing_table, result, tabTerm) = createParseTable()\n",
        "\n",
        "# validate string input using stack-buffer concept\n",
        "if sample_input_string != None:\n",
        "\tvalidity = validateStringUsingStackBuffer(parsing_table, result,\n",
        "\t\t\t\t\t\t\t\t\t\t\ttabTerm, sample_input_string,\n",
        "\t\t\t\t\t\t\t\t\t\t\tterm_userdef,start_symbol)\n",
        "\tprint(validity)\n",
        "else:\n",
        "\tprint(\"\\nNo input String detected\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkzaGToh8iPf",
        "outputId": "78846226-42ba-4732-9107-4167af610de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rules: \n",
            "\n",
            "S->[['A', 'k', 'O']]\n",
            "A->[['A', 'd'], ['a', 'B'], ['a', 'C']]\n",
            "C->[['c']]\n",
            "B->[['b', 'B', 'C'], ['r']]\n",
            "\n",
            "After elimination of left recursion:\n",
            "\n",
            "S->[['A', 'k', 'O']]\n",
            "A->[['a', 'B', \"A'\"], ['a', 'C', \"A'\"]]\n",
            "C->[['c']]\n",
            "B->[['b', 'B', 'C'], ['r']]\n",
            "A'->[['d', \"A'\"], ['#']]\n",
            "\n",
            "After left factoring:\n",
            "\n",
            "S->[['A', 'k', 'O']]\n",
            "A->[['a', \"A''\"]]\n",
            "A''->[['B', \"A'\"], ['C', \"A'\"]]\n",
            "C->[['c']]\n",
            "B->[['b', 'B', 'C'], ['r']]\n",
            "A'->[['d', \"A'\"], ['#']]\n",
            "\n",
            "Calculated firsts: \n",
            "first(S) => {'a'}\n",
            "first(A) => {'a'}\n",
            "first(A'') => {'b', 'r', 'c'}\n",
            "first(C) => {'c'}\n",
            "first(B) => {'b', 'r'}\n",
            "first(A') => {'d', '#'}\n",
            "\n",
            "Calculated follows: \n",
            "follow(S) => {'$'}\n",
            "follow(A) => {'k'}\n",
            "follow(A'') => {'k'}\n",
            "follow(C) => {'k', 'd', 'c'}\n",
            "follow(B) => {'k', 'd', 'c'}\n",
            "follow(A') => {'k'}\n",
            "\n",
            "Firsts and Follow Result table\n",
            "\n",
            "Non-T      FIRST                FOLLOW              \n",
            "S          {'a'}                {'$'}               \n",
            "A          {'a'}                {'k'}               \n",
            "A''        {'b', 'r', 'c'}      {'k'}               \n",
            "C          {'c'}                {'k', 'd', 'c'}     \n",
            "B          {'b', 'r'}           {'k', 'd', 'c'}     \n",
            "A'         {'d', '#'}           {'k'}               \n",
            "\n",
            "Generated parsing table:\n",
            "\n",
            "           k           O           d           a           c           b           r           $\n",
            "S                                         S->A k O                                                \n",
            "A                                         A->a A''                                                \n",
            "A''                                                    A''->C A'   A''->B A'   A''->B A'            \n",
            "C                                                         C->c                                    \n",
            "B                                                                 B->b B C        B->r            \n",
            "A'        A'->#                A'->d A'                                                            \n",
            "\n",
            "Validate String => a r k O\n",
            "\n",
            "              Buffer                Stack               Action\n",
            "           $ O k r a                  S $        T[S][a] = S->A k O\n",
            "           $ O k r a              A k O $        T[A][a] = A->a A''\n",
            "           $ O k r a          a A'' k O $            Matched:a\n",
            "             $ O k r            A'' k O $     T[A''][r] = A''->B A'\n",
            "             $ O k r           B A' k O $            T[B][r] = B->r\n",
            "             $ O k r           r A' k O $            Matched:r\n",
            "               $ O k             A' k O $          T[A'][k] = A'->#\n",
            "               $ O k                k O $            Matched:k\n",
            "                 $ O                  O $            Matched:O\n",
            "                   $                    $                Valid\n",
            "\n",
            "Valid String!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "In this code, I implemented an LL(1) parser in Python that handles grammar rules, eliminates left recursion, performs left factoring, and computes the FIRST and FOLLOW sets for the given grammar. The `removeLeftRecursion` function eliminates left recursion by creating new rules. The `LeftFactoring` function handles left factoring by restructuring rules to remove common prefixes. The `first` and `follow` functions compute the FIRST and FOLLOW sets, essential for parsing table construction. `computeAllFirsts` and `computeAllFollows` functions aggregate these results. The `createParseTable` function constructs the parsing table, checking if the grammar is LL(1). Finally, `validateStringUsingStackBuffer` validates input strings against the constructed LL(1) parsing table, ensuring proper parsing and validation of strings as per the grammar rules."
      ],
      "metadata": {
        "id": "KNUp-1kUER9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Write a program to simulate the parsing process of LR grammar. Take necessary measure to use parsing table."
      ],
      "metadata": {
        "id": "SBkIvHjXCT2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LRParser:\n",
        "    def __init__(self, grammar, parsing_table):\n",
        "        self.grammar = []\n",
        "        for nt, prods in grammar.items():\n",
        "            for p in prods:\n",
        "                self.grammar.append((nt, p))\n",
        "        self.parsing_table = parsing_table\n",
        "        self.stack = []\n",
        "\n",
        "    def parse(self, input_tokens):\n",
        "        # Add end-of-input marker\n",
        "        input_tokens.append('$')\n",
        "        # Initialize stack with state 0\n",
        "        self.stack = [0]\n",
        "        input_pointer = 0\n",
        "        print(f\"{'Stack':<30} {'Input':<20} Action\")\n",
        "        print('-'*30, '-'*20, '-'*6)\n",
        "\n",
        "        while True:\n",
        "            state = self.stack[-1]\n",
        "            lookahead = input_tokens[input_pointer]\n",
        "            action = self.parsing_table['action'][state].get(lookahead)\n",
        "\n",
        "            # Show current stack and input\n",
        "            stack_str = ' '.join(map(str, self.stack))\n",
        "            input_str = ''.join(input_tokens[input_pointer:])\n",
        "            print(f\"{stack_str:<30} {input_str:<20} {action}\")\n",
        "\n",
        "            if action is None:\n",
        "                print()\n",
        "                print(\"Rejected\")\n",
        "                return\n",
        "\n",
        "            if action[0] == 's':  # Shift\n",
        "                next_state = int(action[1:])\n",
        "                self.stack.append(lookahead)\n",
        "                self.stack.append(next_state)\n",
        "                input_pointer += 1\n",
        "            elif action[0] == 'r':  # Reduce\n",
        "                prod_num = int(action[1:])\n",
        "                lhs, rhs = self.grammar[prod_num]\n",
        "                for _ in range(2 * len(rhs)):\n",
        "                    self.stack.pop()\n",
        "                state = self.stack[-1]\n",
        "                self.stack.append(lhs)\n",
        "                self.stack.append(self.parsing_table['goto'][state][lhs])\n",
        "            elif action == 'acc':\n",
        "                print()\n",
        "                print(\"Accepted\")\n",
        "                return\n",
        "            else:\n",
        "                print(\"Error: Invalid action. Check parsing table.\")\n",
        "                return\n",
        "\n",
        "grammar = {  # Reductions\n",
        "    \"S\": [[\"E\"]],                # S -> E 0\n",
        "    \"E\": [[\"E\", \"+\", \"T\"], [\"T\"]],  # E -> E + T | T 1 | 2\n",
        "    \"T\": [[\"T\", \"*\", \"F\"], [\"F\"]],  # T -> T * F | F 3 | 4\n",
        "    \"F\": [[\"(\", \"E\", \")\"], [\"id\"]]  # F -> ( E ) | id 5 | 6\n",
        "}\n",
        "\n",
        "# Parsing table\n",
        "parsing_table = {\n",
        "    'action': {\n",
        "        0: {'id': 's5', '(': 's4'},\n",
        "        1: {'+': 's6', '$': 'acc'},\n",
        "        2: {'+': 'r2', '*': 's7', ')': 'r2', '$': 'r2'},\n",
        "        3: {'+': 'r4', '*': 'r4', ')': 'r4', '$': 'r4'},\n",
        "        4: {'id': 's5', '(': 's4'},\n",
        "        5: {'+': 'r6', '*': 'r6', ')': 'r6', '$': 'r6'},\n",
        "        6: {'id': 's5', '(': 's4'},\n",
        "        7: {'id': 's5', '(': 's4'},\n",
        "        8: {')': 's11', '+': 's6'},\n",
        "        9: {'+': 'r1', '*': 's7', ')': 'r1', '$': 'r1'},\n",
        "        10: {'+': 'r3', '*': 'r3', ')': 'r3', '$': 'r3'},\n",
        "        11: {'+': 'r5', '*': 'r5', ')': 'r5', '$': 'r5'}\n",
        "    },\n",
        "    'goto': {\n",
        "        0: {'E': 1, 'T': 2, 'F': 3},\n",
        "        4: {'E': 8, 'T': 2, 'F': 3},\n",
        "        6: {'T': 9, 'F': 3},\n",
        "        7: {'F': 10}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Input tokens to be parsed\n",
        "input_tokens = ['id', '*', 'id', '+', 'id']\n",
        "print(\"Parsing:\", ''.join(input_tokens))\n",
        "\n",
        "# Create the parser and parse the input string\n",
        "parser = LRParser(grammar, parsing_table)\n",
        "parser.parse(input_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH6AgU_ASIsz",
        "outputId": "93ada8d0-5827-4b8e-89c2-b81daf06588b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing: id*id+id\n",
            "Stack                          Input                Action\n",
            "------------------------------ -------------------- ------\n",
            "0                              id*id+id$            s5\n",
            "0 id 5                         *id+id$              r6\n",
            "0 F 3                          *id+id$              r4\n",
            "0 T 2                          *id+id$              s7\n",
            "0 T 2 * 7                      id+id$               s5\n",
            "0 T 2 * 7 id 5                 +id$                 r6\n",
            "0 T 2 * 7 F 10                 +id$                 r3\n",
            "0 T 2                          +id$                 r2\n",
            "0 E 1                          +id$                 s6\n",
            "0 E 1 + 6                      id$                  s5\n",
            "0 E 1 + 6 id 5                 $                    r6\n",
            "0 E 1 + 6 F 3                  $                    r4\n",
            "0 E 1 + 6 T 9                  $                    r1\n",
            "0 E 1                          $                    acc\n",
            "\n",
            "Accepted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "I implemented a program to simulate the parsing process of LR grammar using a given parsing table. The `LRParser` class initializes with the grammar rules and the parsing table. The `parse` method processes input tokens by utilizing a stack-based approach. It handles actions based on the parsing table, performing shifts, reductions, or accepting the input. The grammar defines productions for expressions and terms, and the parsing table contains actions and state transitions. The method outputs the current state of the stack, input, and the action taken at each step, ultimately determining whether the input string is accepted or rejected according to the LR parsing algorithm."
      ],
      "metadata": {
        "id": "tjoVyw_NSDhX"
      }
    }
  ]
}